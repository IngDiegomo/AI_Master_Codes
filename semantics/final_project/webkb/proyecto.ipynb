{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecnologías Semánticas para la Ciencia del Dato, Proyecto Final\n",
    "\n",
    "Morales Polanco, Diego Enrique\n",
    "\n",
    "Fecha de entrega: 24 de Enero 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto consiste en la implementación de modelos de machine learning junto con métricas relacionales para clasificar las publicaciones del dataset WebKB. Este dataset consiste en \n",
    "877 publicaciones científicas clasificadas en 5 clases (curso, facultad, estudiante, proyecto, personal). Estas publicaciones están relacionadas entre si a traves de citas. Cada publicación es descrita por un vector binario de palabara, el cual indica la presencia o ausencia de una palabra en la publicación. El diccionario de palabras consiste en 1703 palabras únicas de temas relacionados a Machine Learning. Las publicaciones provienen de 4 universidades (cornell, texas, washington, wisconsin).\n",
    "\n",
    "También se pretende comparar los resultados de esta implementación con los resultados de la investigación \"Link Based Classification\" por Qing Lu y Lise Getoor de la Universidad de Maryland, en la cual se utilizó el mismo dataset WebKB, con un modelo de regresión logística. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se importan las funciones y librerias a utilizar para la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "import pprint\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se prepara la data. Se creó la función \"create_graphs\" como función de utilidad para parsear la data de los archivos de cada universidad y crear los grafos correspondientes. Estos grafos se guardan en la variable \"graphs\", cada elemento de esta variable corresponde a un grafo de una universidad, el orden siendo [\"cornell\", \"texas\", \"washington\", \"wisconsin\"]. La variable word_vectors corresponde a los vectores de palabra de cada publicación de cada universidad, en el mismo orden que la variable \"graphs\". La variable classifications indica la clase de cada publicación de cada universidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities = [\"cornell\", \"texas\", \"washington\", \"wisconsin\"]\n",
    "classes = {'course': 0, 'faculty': 1, 'student': 2, 'project': 3, 'staff': 4}\n",
    "graphs = []\n",
    "counts = []\n",
    "classifications = []\n",
    "word_vectors = []\n",
    "\n",
    "for university in universities:\n",
    "\n",
    "    cites_path = university + \".cites\"\n",
    "    content_path = university + \".content\"\n",
    "\n",
    "    with open(cites_path, \"r\") as cites_file:\n",
    "        cites = cites_file.readlines()\n",
    "\n",
    "    with open(content_path, \"r\") as content_file:\n",
    "        content = content_file.readlines()\n",
    "\n",
    "    g = create_graph(content, cites)\n",
    "    graphs.append(g)\n",
    "\n",
    "    count = {'course': 0, 'faculty': 0, 'student': 0, 'project': 0, 'staff': 0}\n",
    "    \n",
    "    y = list(nx.get_node_attributes(g,\"classification\").values())\n",
    "\n",
    "    for element in y:\n",
    "        count[element] += 1\n",
    "    \n",
    "    y = [classes[value] for value in y]\n",
    "\n",
    "    x = list(nx.get_node_attributes(g,\"content\").values())\n",
    "    classifications.append(y)\n",
    "    word_vectors.append(x)\n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se creó la variable counts, que indica cuantas publicaciones de cada clase hay en cada universidad. Esto es para saber que tan balanceado o desbalanceado es el dataset y tomar decisiones a partir de esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 42, 'faculty': 32, 'student': 83, 'project': 19, 'staff': 19},\n",
       " {'course': 34, 'faculty': 31, 'student': 103, 'project': 18, 'staff': 1},\n",
       " {'course': 66, 'faculty': 27, 'student': 107, 'project': 21, 'staff': 9},\n",
       " {'course': 76, 'faculty': 35, 'student': 122, 'project': 22, 'staff': 10}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, el dataset es relativamente desbalanceado. En la universidad \"texas\" por ejemplo, solo hay una publicación de la clase staff, la cual también representa un porcentaje bastante pequeño de las publicaciones en las demas universidades. Se puede considerar eliminara el objeto de la universidad de \"texas\" perteneciente a la clase staff, ya que representa una porción no significativa de la data, pero este elemento no solo tiene un vector de palabras asociado, sino también varias conexiones a otras publicaciones. Por esto, en lugar de eliminar el, se utilizará el método de cross validation kfolds, el cuál mejora el rendimiento en problemas con datasets desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se calculan las métricas relacionales de cada grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [list(nx.algorithms.degree_centrality(g).values()) for g in graphs]\n",
    "betweenness = [list(nx.algorithms.betweenness_centrality(g).values()) for g in graphs]\n",
    "closeness = [list(nx.algorithms.closeness_centrality(g).values()) for g in graphs]\n",
    "eigenvector = [list(nx.algorithms.eigenvector_centrality(g, 500).values()) for g in graphs]\n",
    "page_ranks = [list(nx.pagerank(g).values()) for g in graphs]\n",
    "clustering_coeff = [list(nx.algorithms.cluster.clustering(g).values()) for g in graphs]\n",
    "average_neighbor = [list(nx.average_neighbor_degree(g).values()) for g in graphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea una lista de listas booleanas que representan todas las posibles combinaciones sin repetición que se pueden hacer con las medidas de centralidad calculadas. Esta lista se utilizara para probar cuál combinación de medidas de centralidad dan los mejores resultados de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_measures = ['degrees', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor']\n",
    "parameter_combinations = []\n",
    "for combination in range(1, 2 ** len(centrality_measures)):\n",
    "    params = [bool(combination & (1 << i)) for i in range(len(centrality_measures))]\n",
    "    parameter_combinations.append(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se crea un array que contenga todas las medidas de centralidad de cada grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_metrics = []\n",
    "\n",
    "for i in range (0, len(universities)):\n",
    "    X_metrics.append(np.array([degrees[i], betweenness[i], closeness[i], eigenvector[i], page_ranks[i], clustering_coeff[i], average_neighbor[i]]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se construye un clasificador random forest y se aplica la validación cruzada a cada grafo creado. Solo se utiliza el contenido del vector de palabras como parámetro X del dataset, con el objetivo de comparar el rendimiento al introducir los parámetros relacionales. Se utilizan las mismas métricas de rendimiento que en la investigación \"Link Based Classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell university:\n",
      "  Avg. Accuracy = 0.712,\n",
      "  Avg. F1 = 0.652,\n",
      "  Avg. Precision = 0.635,\n",
      "  Avg. Recall = 0.712\n",
      "Texas university:\n",
      "  Avg. Accuracy = 0.776,\n",
      "  Avg. F1 = 0.715,\n",
      "  Avg. Precision = 0.721,\n",
      "  Avg. Recall = 0.776\n",
      "Washington university:\n",
      "  Avg. Accuracy = 0.817,\n",
      "  Avg. F1 = 0.762,\n",
      "  Avg. Precision = 0.735,\n",
      "  Avg. Recall = 0.817\n",
      "Wisconsin university:\n",
      "  Avg. Accuracy = 0.811,\n",
      "  Avg. F1 = 0.763,\n",
      "  Avg. Precision = 0.741,\n",
      "  Avg. Recall = 0.811\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators= 1000, criterion= \"gini\", random_state= 10052)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        score = cross_validate(clf, np.array(word_vectors[i]), classifications[i], scoring=scoring, cv=10)\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "for i in range(0, len(universities)):\n",
    "    print(f'{universities[i].capitalize()} university:\\n  Avg. Accuracy = {np.mean(scores[i][\"test_accuracy\"]):.3f},\\n  Avg. F1 = {np.mean(scores[i][\"test_f1_weighted\"]):.3f},\\n  Avg. Precision = {np.mean(scores[i][\"test_precision_weighted\"]):.3f},\\n  Avg. Recall = {np.mean(scores[i][\"test_recall_weighted\"]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye un clasificador de random forest y se aplica la validación cruzada, pero ahora utilizando una de los parámetros relacionales calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell university:\n",
      "  Avg. Accuracy = 0.727,\n",
      "  Avg. F1 = 0.669,\n",
      "  Avg. Precision = 0.651,\n",
      "  Avg. Recall = 0.727\n",
      "Texas university:\n",
      "  Avg. Accuracy = 0.797,\n",
      "  Avg. F1 = 0.743,\n",
      "  Avg. Precision = 0.752,\n",
      "  Avg. Recall = 0.797\n",
      "Washington university:\n",
      "  Avg. Accuracy = 0.817,\n",
      "  Avg. F1 = 0.762,\n",
      "  Avg. Precision = 0.735,\n",
      "  Avg. Recall = 0.817\n",
      "Wisconsin university:\n",
      "  Avg. Accuracy = 0.815,\n",
      "  Avg. F1 = 0.770,\n",
      "  Avg. Precision = 0.757,\n",
      "  Avg. Recall = 0.815\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators= 1000, criterion= \"gini\", random_state= 10052)\n",
    "\n",
    "    x = np.hstack((np.array(word_vectors[i]), np.array(eigenvector[i]).reshape(-1, 1)))\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        score = cross_validate(estimator= clf, X = x, y= classifications[i], scoring=scoring, cv=10)\n",
    "        \n",
    "    scores.append(score)\n",
    "\n",
    "for i in range(0, len(universities)):\n",
    "    print(f'{universities[i].capitalize()} university:\\n  Avg. Accuracy = {np.mean(scores[i][\"test_accuracy\"]):.3f},\\n  Avg. F1 = {np.mean(scores[i][\"test_f1_weighted\"]):.3f},\\n  Avg. Precision = {np.mean(scores[i][\"test_precision_weighted\"]):.3f},\\n  Avg. Recall = {np.mean(scores[i][\"test_recall_weighted\"]):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia, no se aprecia una mejora en el rendimiento, de hecho, utilizando solo el vector de palabras se obtiene un mejor rendimiento en la mayoría de los scores que añadiendo el parámetro de eigenvector. A continuación se hará finetuning con RandomGridSearch al modelo con solo el vector de palabras, para encontrar los mejores parámetros, también se probarán las distintas combinaciones de métricas relacionales posibles y se encontrarán los parámetros que consigan los mejores resultados, también se hará finetuning a la combinación de métricas encontrada.\n",
    "\n",
    "Primero se hará un randomized search en un rango grande de hiperparámetros, luego se explorará más a profundidad rangos cercanos a los encontrados por el randomized search inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
       " 'max_features': ['sqrt', 'auto', None, 0.3],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 42, 83, 124, 165, 206, 246, 287, 328, 369, 410],\n",
       " 'min_samples_leaf': [1, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110],\n",
       " 'bootstrap': [True, False],\n",
       " 'criterion': ['gini', 'entropy', 'log_loss'],\n",
       " 'max_leaf_nodes': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_impurity_decrease': [0.0, 0.0001, 0.001, 0.01, 0.1],\n",
       " 'oob_score': [True, False]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = [ 'sqrt', 'auto', None, 0.3]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 410, num = 11)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 110, num = 11)]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['gini', 'entropy', \"log_loss\"]\n",
    "max_leaf_nodes = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_leaf_nodes.append(None)\n",
    "min_impurity_decrease = [0.0, 0.0001, 0.001, 0.01, 0.1]\n",
    "oob_score = [True, False]\n",
    "\n",
    "\n",
    "random_wide_grid = {'n_estimators': n_estimators,\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'bootstrap': bootstrap,\n",
    "                    'criterion': criterion,\n",
    "                    'max_leaf_nodes': max_leaf_nodes,\n",
    "                    'min_impurity_decrease': min_impurity_decrease,\n",
    "                    'oob_score': oob_score}\n",
    "random_wide_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Cornell university:\n",
      "0.7005640164397542\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Texas university:\n",
      "0.8036323662776752\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 40,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 20,\n",
      " 'min_impurity_decrease': 0.001,\n",
      " 'min_samples_leaf': 11,\n",
      " 'min_samples_split': 42,\n",
      " 'n_estimators': 1400,\n",
      " 'oob_score': False}\n",
      " Washington university:\n",
      "0.787505766765542\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Wisconsin university:\n",
      "0.7934207096478035\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_wide_grid, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(np.array(word_vectors[i]), classifications[i])    \n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que los scores F1 mejoraron considerablemente en los 4 grafos. Ahora se procede a realizar un random grid search explorando más a profundidad la cercania de los valores obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [400, 600, 800, 1000],\n",
       " 'max_features': ['sqrt', 0.3, None],\n",
       " 'max_depth': [20, 40, 60, 90, 100, 120],\n",
       " 'min_samples_split': [1, 2, 5, 30, 42, 50],\n",
       " 'min_samples_leaf': [1, 2, 5, 11, 15, 18],\n",
       " 'bootstrap': [True, False],\n",
       " 'criterion': ['entropy', 'gini'],\n",
       " 'max_leaf_nodes': [10, 20, 50, 80, 100, 120],\n",
       " 'min_impurity_decrease': [0.0001, 0.001, 0.0],\n",
       " 'oob_score': [True, False]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [400, 600, 800, 1000]\n",
    "max_features = ['sqrt', 0.3, None]\n",
    "max_depth = [20, 40, 60, 90, 100, 120]\n",
    "min_samples_split = [1, 2, 5, 30, 42, 50]\n",
    "min_samples_leaf = [1, 2, 5, 11, 15, 18]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['entropy', 'gini']\n",
    "max_leaf_nodes = [10, 20, 50, 80, 100, 120]\n",
    "min_impurity_decrease = [0.0001, 0.001, 0.0]\n",
    "oob_score = [True, False]\n",
    "\n",
    "random_narrow_grid = {  'n_estimators': n_estimators,\n",
    "                        'max_features': max_features,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf,\n",
    "                        'bootstrap': bootstrap,\n",
    "                        'criterion': criterion,\n",
    "                        'max_leaf_nodes': max_leaf_nodes,\n",
    "                        'min_impurity_decrease': min_impurity_decrease,\n",
    "                         'oob_score': oob_score}\n",
    "random_narrow_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 100,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 120,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Cornell university:\n",
      "0.7449254321455155\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 100,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 120,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Texas university:\n",
      "0.7667203235361131\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 120,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 50,\n",
      " 'min_impurity_decrease': 0.001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 42,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Washington university:\n",
      "0.7967430807913172\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 100,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 120,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Wisconsin university:\n",
      "0.8216822255821754\n"
     ]
    }
   ],
   "source": [
    "word_vector_fine_tuned_params = []          # Variable para guardar los mejores parámetros encontrados utilizando solo el vector de palabras\n",
    "\n",
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_narrow_grid, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(np.array(word_vectors[i]), classifications[i])    \n",
    "    \n",
    "    word_vector_fine_tuned_params.append(random_search.best_params_)\n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se procede a encontrar la combinación de métricas relacionales que de el mejor resultado de rendimiento para luego hacerle finetuning a los parámetros de la combinación encontrada, asi como se hizo para el vector de palabras. Recordando que la variable X metrics es una lista de arrays cuyas columnas son las medidas de centralidad en el orden 'degrees', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor' de cada grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia se intentó realizar un grid search a cada combinación de métricas relacionales, pero el tiempo y la carga computacional de esto resultaba demasiado elevada. Por esto se decidió comentar el código y se probaran random forests con los parámetros default y una validación cruzada de 10 folds para todas las combinaciones, luego se hará un finetuning a la mejor combinación encontrada.\n",
    "\n",
    "También se pudo apreciar que las métricas relacionales por si solas dan resultados mucho menor que 50%, por lo que solo se imprimiran y se guardarán las combinaciones con resultados mayores a cierto threshold. Más adelante, luego de realizar finetuning, se combinaran las métricas relacionales con el vector de palabras para ver su resultado conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell university: \n",
      "Mejores Scores: (0.496613488850331, 0.496208001945007, 0.4937696522603644, 0.4885837450714286, 0.4875150944957854) \n",
      "Mejores parámetros: (['betweenness', 'pagerank', 'clustering', 'average_neighbor'], ['degrees', 'betweenness', 'pagerank', 'average_neighbor'], ['betweenness', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor'], ['degrees', 'betweenness', 'pagerank', 'clustering'], ['degrees', 'eigenvector', 'pagerank', 'average_neighbor']) \n",
      "Indices: ([1, 4, 5, 6], [0, 1, 4, 6], [1, 3, 4, 5, 6], [0, 1, 4, 5], [0, 3, 4, 6])\n",
      "\n",
      "Texas university: \n",
      "Mejores Scores: (0.6404649524557992, 0.6333659907344118, 0.6286020918934726, 0.6260747957582435, 0.6199120385504826) \n",
      "Mejores parámetros: (['degrees', 'betweenness', 'closeness', 'eigenvector', 'clustering', 'average_neighbor'], ['degrees', 'betweenness', 'closeness', 'clustering', 'average_neighbor'], ['betweenness', 'closeness', 'eigenvector', 'clustering', 'average_neighbor'], ['degrees', 'betweenness', 'pagerank', 'clustering', 'average_neighbor'], ['degrees', 'closeness', 'average_neighbor']) \n",
      "Indices: ([0, 1, 2, 3, 5, 6], [0, 1, 2, 5, 6], [1, 2, 3, 5, 6], [0, 1, 4, 5, 6], [0, 2, 6])\n",
      "\n",
      "Washington university: \n",
      "Mejores Scores: (0.524118702204907, 0.5010477454646665, 0.4999595523957662, 0.4965512356356346, 0.49421369213996813) \n",
      "Mejores parámetros: (['degrees', 'closeness', 'pagerank'], ['degrees', 'betweenness', 'eigenvector', 'pagerank', 'average_neighbor'], ['degrees', 'closeness', 'eigenvector', 'pagerank'], ['degrees', 'closeness', 'eigenvector', 'pagerank', 'average_neighbor'], ['degrees', 'betweenness', 'closeness', 'clustering']) \n",
      "Indices: ([0, 2, 4], [0, 1, 3, 4, 6], [0, 2, 3, 4], [0, 2, 3, 4, 6], [0, 1, 2, 5])\n",
      "\n",
      "Wisconsin university: \n",
      "Mejores Scores: (0.5179135411856504, 0.5157144967654941, 0.5132647843691169, 0.5121641253854169, 0.5112193336336964) \n",
      "Mejores parámetros: (['betweenness', 'closeness', 'average_neighbor'], ['betweenness', 'closeness', 'clustering', 'average_neighbor'], ['betweenness', 'closeness', 'clustering'], ['degrees', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor'], ['betweenness', 'closeness', 'eigenvector', 'average_neighbor']) \n",
      "Indices: ([1, 2, 6], [1, 2, 5, 6], [1, 2, 5], [0, 1, 2, 3, 4, 5, 6], [1, 2, 3, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_measures = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "best_measures_indexes = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "best_scores = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "i = 0\n",
    "\n",
    "for university in universities:\n",
    "\n",
    "    for combination in parameter_combinations:\n",
    "\n",
    "        measures = [centrality_measures[j] for j, include_measure in enumerate(combination) if include_measure]\n",
    "        indexes = [j for j, include_measure in enumerate(combination) if include_measure]\n",
    "\n",
    "        X_to_try = X_metrics[i][: , indexes]\n",
    "        \n",
    "        clf = RandomForestClassifier(random_state= 10052)\n",
    "        #random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_wide_grid, n_iter= 100, cv= 5, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            score = cross_validate(clf, X_to_try, classifications[i], scoring=scoring, cv=10)\n",
    "            #random_search.fit(X_to_try, classifications[i])  \n",
    "\n",
    "        if np.mean(score[\"test_f1_weighted\"]) > 0.45:\n",
    "            #print(f'\\n{university.capitalize()} university: Metricas: {measures}, Avg_F1_Score: {np.mean(score[\"test_f1_weighted\"])}')\n",
    "            best_measures[university].append(measures)\n",
    "            best_measures_indexes[university].append(indexes)\n",
    "            best_scores[university].append(np.mean(score[\"test_f1_weighted\"]))\n",
    "            \n",
    "    combined = list(zip(best_scores[university], best_measures[university], best_measures_indexes[university]))\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    sorted_scores, sorted_parameters, sorted_indexes = zip(*sorted_combined)\n",
    "\n",
    "    print(f'{university.capitalize()} university: \\nMejores Scores: {sorted_scores[:5]} \\nMejores parámetros: {sorted_parameters[:5]} \\nIndices: {sorted_indexes[:5]}\\n')\n",
    "    i = 1 + i  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se procede a hacer el mismo experimento pero introduciendo el vector de palabras junto con las combinaciones de las métricas relacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell university: \n",
      "Mejores Scores: (0.6992677916881518, 0.6937679176010201, 0.6904978431012696, 0.6889780322017165, 0.6884253302596955) \n",
      "Mejores parámetros: (['closeness', 'pagerank', 'clustering'], ['degrees', 'betweenness', 'closeness', 'pagerank'], ['degrees', 'betweenness', 'closeness', 'pagerank', 'clustering', 'average_neighbor'], ['closeness', 'pagerank', 'average_neighbor'], ['betweenness', 'closeness', 'pagerank', 'clustering', 'average_neighbor']) \n",
      "Indices: ([2, 4, 5], [0, 1, 2, 4], [0, 1, 2, 4, 5, 6], [2, 4, 6], [1, 2, 4, 5, 6])\n",
      "\n",
      "Texas university: \n",
      "Mejores Scores: (0.7607448249690812, 0.7591068697384487, 0.7555116637748216, 0.7536511149669045, 0.75283476951898) \n",
      "Mejores parámetros: (['degrees', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor'], ['degrees', 'clustering', 'average_neighbor'], ['degrees', 'closeness'], ['degrees', 'betweenness', 'average_neighbor'], ['degrees', 'betweenness', 'closeness', 'eigenvector', 'pagerank']) \n",
      "Indices: ([0, 3, 4, 5, 6], [0, 5, 6], [0, 2], [0, 1, 6], [0, 1, 2, 3, 4])\n",
      "\n",
      "Washington university: \n",
      "Mejores Scores: (0.7695423113608368, 0.7689480664263273, 0.7651755201320418, 0.7651755201320418, 0.7636661744053047) \n",
      "Mejores parámetros: (['degrees', 'betweenness', 'closeness', 'eigenvector', 'clustering', 'average_neighbor'], ['degrees', 'eigenvector', 'pagerank'], ['betweenness', 'closeness', 'eigenvector', 'pagerank', 'clustering'], ['betweenness', 'closeness', 'eigenvector', 'pagerank', 'average_neighbor'], ['eigenvector', 'pagerank', 'clustering']) \n",
      "Indices: ([0, 1, 2, 3, 5, 6], [0, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4, 6], [3, 4, 5])\n",
      "\n",
      "Wisconsin university: \n",
      "Mejores Scores: (0.7855925560749955, 0.7840112099759571, 0.7839804365676837, 0.7838972367684596, 0.7828341940603791) \n",
      "Mejores parámetros: (['degrees', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'average_neighbor'], ['eigenvector'], ['degrees', 'closeness', 'eigenvector', 'pagerank', 'clustering', 'average_neighbor'], ['pagerank', 'average_neighbor'], ['eigenvector', 'clustering']) \n",
      "Indices: ([0, 1, 2, 3, 4, 6], [3], [0, 2, 3, 4, 5, 6], [4, 6], [3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_measures = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "best_measures_indexes = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "best_scores = {\"cornell\": [], \"texas\": [], \"washington\":[], \"wisconsin\":[]}\n",
    "i = 0\n",
    "\n",
    "for university in universities:\n",
    "\n",
    "    for combination in parameter_combinations:\n",
    "\n",
    "        measures = [centrality_measures[j] for j, include_measure in enumerate(combination) if include_measure]\n",
    "        indexes = [j for j, include_measure in enumerate(combination) if include_measure]\n",
    "\n",
    "        X_to_try = X_metrics[i][: , indexes]\n",
    "        X_combined = np.hstack((np.array(word_vectors[i]), X_to_try))\n",
    "        clf = RandomForestClassifier(random_state= 10052)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            score = cross_validate(clf, X_combined, classifications[i], scoring=scoring, cv=10)\n",
    "\n",
    "        if np.mean(score[\"test_f1_weighted\"]) > 0.45:\n",
    "            best_measures_indexes[university].append(indexes)\n",
    "            best_measures[university].append(measures)\n",
    "            best_scores[university].append(np.mean(score[\"test_f1_weighted\"]))\n",
    "            \n",
    "    combined = list(zip(best_scores[university], best_measures[university], best_measures_indexes[university]))\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    sorted_scores, sorted_parameters, sorted_indexes = zip(*sorted_combined)\n",
    "\n",
    "    print(f'{university.capitalize()} university: \\nMejores Scores: {sorted_scores[:5]} \\nMejores parámetros: {sorted_parameters[:5]} \\nIndices: {sorted_indexes[:5]}\\n')\n",
    "    i = 1 + i  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya obtenidas las mejores combinaciones de métricas relacionales se procede a hacer el mismo proceso de finetuning. Se hará un proceso para datasets creados únicamente con las métricas relacionales y otro proceso para datasets creados con las métricas relacionales más el vector de palabras. Primero se crean los datasets utilizando los indices de las mejores combinaciones de métricas encontradas para cada universidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_only_indexes = [[1, 4, 5, 6], [0, 1, 2, 3, 5, 6], [0, 2, 4], [1, 2, 6]]\n",
    "best_metric_and_vector_indexes = [[2, 4, 5], [0, 3, 4, 5, 6], [0, 1, 2, 3, 5, 6], [0, 1, 2, 3, 4, 6]]\n",
    "X_metric_only = []\n",
    "X_metric_and_vector = []\n",
    "i = 0\n",
    "\n",
    "for indexes in best_metric_only_indexes:\n",
    "    X_metric_only.append(X_metrics[i][: , indexes])\n",
    "    i = i + 1\n",
    "\n",
    "i = 0\n",
    "\n",
    "for indexes in best_metric_and_vector_indexes:\n",
    "    metrics_columns = X_metrics[i][: , indexes]\n",
    "    X_combined = np.hstack((np.array(word_vectors[i]), metrics_columns))\n",
    "    X_metric_and_vector.append(X_combined)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelve a utilizar el grid amplio de busqueda de parámetros para luego hacer una búsqueda más focalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Cornell university:\n",
      "0.49907545799573666\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Texas university:\n",
      "0.6228393695041293\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Washington university:\n",
      "0.5266158351205611\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Wisconsin university:\n",
      "0.5077921464941308\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_wide_grid, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(X_metric_only[i] , classifications[i])    \n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente se encontraron hiperparámetros parecidos para los 4 grafos. Se aplicará la una grid distinta a la del finetuning anterior ya que esta vez los 4 grafos arrojaron exactamente los mismos hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [400, 600, 800, 1000],\n",
       " 'max_features': ['sqrt', 0.3, None],\n",
       " 'max_depth': [70, 80, 90, 100, 110],\n",
       " 'min_samples_split': [1, 2, 5, 8],\n",
       " 'min_samples_leaf': [1, 2, 5],\n",
       " 'bootstrap': [True, False],\n",
       " 'criterion': ['entropy', 'gini'],\n",
       " 'max_leaf_nodes': [80, 100, 120],\n",
       " 'min_impurity_decrease': [0.0001, 0.001, 0.0],\n",
       " 'oob_score': [True, False]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [400, 600, 800, 1000]\n",
    "max_features = ['sqrt', 0.3, None]\n",
    "max_depth = [70, 80, 90, 100, 110]\n",
    "min_samples_split = [1, 2, 5, 8]\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['entropy', 'gini']\n",
    "max_leaf_nodes = [80, 100, 120]\n",
    "min_impurity_decrease = [0.0001, 0.001, 0.0]\n",
    "oob_score = [True, False]\n",
    "\n",
    "random__narrow_grid_2 = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion': criterion,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'min_impurity_decrease': min_impurity_decrease,\n",
    "               'oob_score': oob_score}\n",
    "\n",
    "random__narrow_grid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 8,\n",
      " 'n_estimators': 400,\n",
      " 'oob_score': False}\n",
      " Cornell university:\n",
      "0.5124263232738872\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 8,\n",
      " 'n_estimators': 400,\n",
      " 'oob_score': False}\n",
      " Texas university:\n",
      "0.6298609226183597\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 400,\n",
      " 'oob_score': False}\n",
      " Washington university:\n",
      "0.5566180824624058\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 110,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 80,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Wisconsin university:\n",
      "0.5167801685093741\n"
     ]
    }
   ],
   "source": [
    "metrics_only_tuned_params = []          # Variable para guardar los mejores parámetros encontrados utilizando solo las métricas relacionales\n",
    "\n",
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random__narrow_grid_2, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(X_metric_only[i] , classifications[i])    \n",
    "    \n",
    "    metrics_only_tuned_params.append(random_search.best_params_)\n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se han obtenido los mejores hiperparámetros utilizando solamente las métricas relacionales. Ahora se procede a realizar el finetuning para el dataset que incluye las métricas relacionales y el vector de palabras a la vez. Nuevamente se utiliza el grid amplio de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Cornell university:\n",
      "0.6919972479044502\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Texas university:\n",
      "0.7785916364337416\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 40,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 20,\n",
      " 'min_impurity_decrease': 0.001,\n",
      " 'min_samples_leaf': 11,\n",
      " 'min_samples_split': 42,\n",
      " 'n_estimators': 1400,\n",
      " 'oob_score': False}\n",
      " Washington university:\n",
      "0.787505766765542\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 90,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': 100,\n",
      " 'min_impurity_decrease': 0.0001,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 600,\n",
      " 'oob_score': False}\n",
      " Wisconsin university:\n",
      "0.8140289725357599\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_wide_grid, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(X_metric_and_vector[i] , classifications[i])    \n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente se encontraron parametros parecidos para los 4 grafos. Esto puede deberse a que este conjunto de parámetros se adapta bien a la estructura de citas y vectores de palabra que tienen los grafos con lo que se está trabajando. Esta vez se encontraron exactamente los mismos parámetros que utilizando solo el vector de palabras, por lo que se usará el grid narrow 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 120,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 20,\n",
      " 'min_impurity_decrease': 0.001,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 800,\n",
      " 'oob_score': False}\n",
      " Cornell university:\n",
      "0.7447484572531012\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': False,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 120,\n",
      " 'max_features': 0.3,\n",
      " 'max_leaf_nodes': 10,\n",
      " 'min_impurity_decrease': 0.001,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': False}\n",
      " Texas university:\n",
      "0.7618242449558239\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 100,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 120,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Washington university:\n",
      "0.8010582201932316\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'entropy',\n",
      " 'max_depth': 100,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': 120,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 5,\n",
      " 'n_estimators': 1000,\n",
      " 'oob_score': True}\n",
      " Wisconsin university:\n",
      "0.8120023662447476\n"
     ]
    }
   ],
   "source": [
    "vector_and_metrics_tuned_params = []          # Variable para guardar los mejores parámetros encontrados utilizando el vector de palabras y las métricas relacionales a la vez\n",
    "for i in range(0,len(universities)):\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    random_search = RandomizedSearchCV(estimator=clf, param_distributions= random_narrow_grid, n_iter= 100, cv= 10, verbose= 1, random_state= 10052, scoring=scoring, refit=\"f1_weighted\", n_jobs=-1)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        random_search.fit(np.array(X_metric_and_vector[i]), classifications[i])    \n",
    "    \n",
    "    vector_and_metrics_tuned_params.append(random_search.best_params_)\n",
    "    pprint.pprint(random_search.best_params_)\n",
    "    print(f' {universities[i].capitalize()} university:')\n",
    "    pprint.pprint(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se entrenará un Random Forest con los mejores hiperparámetros de cada dataset (vector de palabras, métricas relacionales y combinación de ambos) para una última comparación en base a la cuál realizar conclusiones en la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [word_vector_fine_tuned_params, metrics_only_tuned_params ,vector_and_metrics_tuned_params]\n",
    "model_names = ['Solo vector de palabras', 'Solo métricas relacionales', 'Vector y métricas juntos:']\n",
    "datasets = [word_vectors, X_metric_only, X_metric_and_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell University:\n",
      "\n",
      "    Solo vector de palabras:\n",
      "    Avg. Accuracy = 0.754, Avg. F1 = 0.730, Avg. Precision = 0.747,  Avg. Recall = 0.754\n",
      "\n",
      "    Solo métricas relacionales:\n",
      "    Avg. Accuracy = 0.565, Avg. F1 = 0.512, Avg. Precision = 0.500,  Avg. Recall = 0.565\n",
      "\n",
      "    Vector y métricas juntos::\n",
      "    Avg. Accuracy = 0.763, Avg. F1 = 0.739, Avg. Precision = 0.750,  Avg. Recall = 0.763\n",
      "\n",
      "Texas University:\n",
      "\n",
      "    Solo vector de palabras:\n",
      "    Avg. Accuracy = 0.787, Avg. F1 = 0.754, Avg. Precision = 0.764,  Avg. Recall = 0.787\n",
      "\n",
      "    Solo métricas relacionales:\n",
      "    Avg. Accuracy = 0.670, Avg. F1 = 0.634, Avg. Precision = 0.637,  Avg. Recall = 0.670\n",
      "\n",
      "    Vector y métricas juntos::\n",
      "    Avg. Accuracy = 0.803, Avg. F1 = 0.766, Avg. Precision = 0.769,  Avg. Recall = 0.803\n",
      "\n",
      "Washington University:\n",
      "\n",
      "    Solo vector de palabras:\n",
      "    Avg. Accuracy = 0.796, Avg. F1 = 0.797, Avg. Precision = 0.833,  Avg. Recall = 0.796\n",
      "\n",
      "    Solo métricas relacionales:\n",
      "    Avg. Accuracy = 0.574, Avg. F1 = 0.532, Avg. Precision = 0.520,  Avg. Recall = 0.574\n",
      "\n",
      "    Vector y métricas juntos::\n",
      "    Avg. Accuracy = 0.822, Avg. F1 = 0.801, Avg. Precision = 0.803,  Avg. Recall = 0.822\n",
      "\n",
      "Wisconsin University:\n",
      "\n",
      "    Solo vector de palabras:\n",
      "    Avg. Accuracy = 0.835, Avg. F1 = 0.812, Avg. Precision = 0.808,  Avg. Recall = 0.835\n",
      "\n",
      "    Solo métricas relacionales:\n",
      "    Avg. Accuracy = 0.547, Avg. F1 = 0.514, Avg. Precision = 0.511,  Avg. Recall = 0.547\n",
      "\n",
      "    Vector y métricas juntos::\n",
      "    Avg. Accuracy = 0.838, Avg. F1 = 0.810, Avg. Precision = 0.803,  Avg. Recall = 0.838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(universities)):\n",
    "    \n",
    "    print(f'{universities[i].capitalize()} University:\\n')\n",
    "    \n",
    "    for j in range(0,len(models)):\n",
    "\n",
    "        clf = RandomForestClassifier(**models[j][i], random_state= 10052)\n",
    "    \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            score = cross_validate(clf, np.array(datasets[j][i]), classifications[i], scoring=scoring, cv=10)\n",
    "        \n",
    "        print(f'    {model_names[j]}:')\n",
    "        print(f'    Avg. Accuracy = {np.mean(score[\"test_accuracy\"]):.3f}, Avg. F1 = {np.mean(score[\"test_f1_weighted\"]):.3f}, Avg. Precision = {np.mean(score[\"test_precision_weighted\"]):.3f},  Avg. Recall = {np.mean(score[\"test_recall_weighted\"]):.3f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
